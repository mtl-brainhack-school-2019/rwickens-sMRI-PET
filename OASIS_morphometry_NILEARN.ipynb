{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis jupyter notebook includes code taken from the following nilearn project, \\n\"8.3.10. Voxel-Based Morphometry on Oasis dataset\", which predicts age from\\ngrey matter morhpometry. Feature redux = k-best ANOVA, prediction function = SVM. \\n\\nI am tweaking the code to the following analysis: \\nInclude train_test_split\\nFeature redux = PCA, prediction function = SVM. \\nI\\'ve included various plots.\\nI\\'m having some trouble using the pipeline function for PCA and SVM. \\nWill implement k-fold validation\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This jupyter notebook includes code taken from the following nilearn project, \n",
    "\"8.3.10. Voxel-Based Morphometry on Oasis dataset\", which predicts age from\n",
    "grey matter morhpometry. Feature redux = k-best ANOVA, prediction function = SVM. \n",
    "\n",
    "I am tweaking the code to add the following:  \n",
    "Include train_test_split\n",
    "k-fold validation\n",
    "Feature redux = PCA, prediction function = SVM. \n",
    "Various plots. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's keep our notebook clean, so it's a little more readable!\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn.input_data import NiftiMasker\n",
    "#import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rwick\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py:2315: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
      "  output = genfromtxt(fname, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "oasis_dataset = datasets.fetch_oasis_vbm() #Selected all 403 subjects\n",
    "gray_matter_map_filenames = oasis_dataset.gray_matter_maps\n",
    "age = oasis_dataset.ext_vars['age'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#for later if I decide to look at white matter\\n#Note to self: make sure the nifti masker is adapted for white matter, with correct fwhm\\nwhite_matter_map_filenames = oasis_dataset.white_matter_maps\\nwm_maps_masked = nifti_masker.fit_transform(white_matter_map_filenames)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#for later if I decide to look at white matter\n",
    "#Note to self: make sure the nifti masker is adapted for white matter, with correct fwhm\n",
    "white_matter_map_filenames = oasis_dataset.white_matter_maps\n",
    "wm_maps_masked = nifti_masker.fit_transform(white_matter_map_filenames)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-72ee2e15c7ee>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-72ee2e15c7ee>\"\u001b[1;36m, line \u001b[1;32m12\u001b[0m\n\u001b[1;33m    np.save(\"C:/Users/rwick/Documents/GitHub/rwickens-sMRI-PET/oasis.npy\", gm_maps_masked_nondisk)\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "nifti_masker = NiftiMasker(\n",
    "    standardize=False,\n",
    "    smoothing_fwhm=2,\n",
    "    memory='nilearn_cache')  # cache options\n",
    "\n",
    "gm_maps_masked_nondisk = nifti_masker.fit_transform(gray_matter_map_filenames)\n",
    "\n",
    "n_samples, n_features = gm_maps_masked_nondisk.shape\n",
    "\n",
    "print(\"%d samples, %d features\" % (n_samples, n_features)\n",
    "      \n",
    "np.save(\"C:/Users/rwick/Documents/GitHub/rwickens-sMRI-PET/oasis.npy\", gm_maps_masked_nondisk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load array from disk\n",
    "gm_maps_masked = np.load(\"C:\\Users\\rwick\\Documents\\GitHub\\rwickens-sMRI-PET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gm_maps_masked\n",
    "#np.range(gm_maps_masked)\n",
    "#def range(x, axis=0):\n",
    "    #return np.max(x, axis=axis) - np.min(x, axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.amax(gm_maps_masked)\n",
    "#Max and min, and tissue values aren't make sense if they're supposed to represent probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.amin(gm_maps_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subject1 = oasis_dataset.gray_matter_maps[0]\n",
    "plotting.view_img(subject1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, running a PCA instead of the K-best method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"PCA + SVR\")\n",
    "\n",
    "'''\n",
    "#If PCA is supposed to be done before train test split, \n",
    "#pca = PCA(n_components=100)\n",
    "#GM_compressed = pca.fit_transform(gm_maps_masked) # Fit the data for the training set \n",
    "'''\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "GMtrain, GMtest, age_train, age_test = train_test_split(gm_maps_masked, age, random_state=1)\n",
    "\n",
    "# Define the prediction function to be used.\n",
    "# Here we use a Support Vector Classification, with a linear kernel\n",
    "from sklearn.svm import SVR\n",
    "svr = SVR(kernel='linear')\n",
    "\n",
    "# Dimension reduction\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#scaler = MinMaxScaler()\n",
    "#scaler.tranform(GMtrain)\n",
    "#scaler.tranform(GMtest)\n",
    "#doesn't work\n",
    "pca = PCA(n_components=100)\n",
    "GMtrain_compressed = pca.fit_transform(GMtrain) # Fit the data for the training set \n",
    "GMtest_compressed = pca.fit_transform(GMtest) # Fit the data for the test set\n",
    "\n",
    "#Does fit_transform take care of scaling? \n",
    "#Exact difference between fit and fit_transform?\n",
    "#This is basically doing the feature selection, right? \n",
    "\n",
    "#Plotting the Cumulative Summation of the Explained Variance\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') #for each component\n",
    "plt.title('PCA - Oasis Grey Matter Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_maps_masked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GMtrain_compressed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GMtest_compressed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortened_components=pca.explained_variance_ratio_[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shortened_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist = []\n",
    "for i in range(20): \n",
    "    mylist.append(\"PCA%i\" % i)\n",
    "mylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creating a plot showing variance explained by first 20 components. \n",
    "plt.figure(figsize=(15,5))\n",
    "df = pd.DataFrame({'var':shortened_components,\n",
    "             'PC':mylist})\n",
    "sns.barplot(x='PC',y=\"var\", \n",
    "           data=df, color=\"c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When I tried running a lineplot, my components got all out of order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PCA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-bae076804603>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m pipe = Pipeline([\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[1;34m'PCA'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m123\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     ('svr', svr)])                                                   \n\u001b[0;32m      5\u001b[0m \u001b[1;31m### Fit and predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PCA' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([\n",
    "    ('PCA', PCA(random_state=123)), \n",
    "    ('svr', svr)])                                                   \n",
    "### Fit and predict\n",
    "pipefit = pipe.fit(GMtrain, age_train)\n",
    "#any use of storing this into a varaible? \n",
    "age_pred_reb = pipe.predict(GMtest)\n",
    "#age_pred_reb = pipefit.predict(GMtest) - should it be this? \n",
    "\n",
    "\n",
    "#Running cross-validation on test-data\n",
    "type(age_pred_reb)\n",
    "cv_scores_train = cross_val_score(pipe, GMtrain, age_train)\n",
    "\n",
    "#Other notation: label = model.fit(X, y).predict(X)\n",
    "\n",
    "#from sklearn.metrics import accuracy_score\n",
    "#this looks at y-hat minus y I assume\n",
    "#accuracy_score(ytest, y_model)\n",
    "#accuracy_train = accuracy_score(GMtrain, pipefit(GMtrain)) ?) \n",
    "#accuracy_test = accuracy_score(ytrain, pipefit(GMtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure accuracy with cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#Check how to do cross-validation scores with train test split\n",
    "#Can I be sure that the same parameters are used for the training as they are for testing? \n",
    "#Where is the information being stored in memory? \n",
    "cv_scores_test = cross_val_score(pipe, GMtest, age_test)\n",
    "\n",
    "#I'd be interested in finding the ordinary least squares between predicted and true age\n",
    "#for x in range(y):\n",
    "#loop thru (age - age_pred_reb)^2\n",
    "\n",
    "# Return the corresponding mean prediction accuracy\n",
    "prediction_accuracy = np.mean(cv_scores)\n",
    "print(\"=== SVM ===\")\n",
    "print(\"Prediction accuracy: %f\" % prediction_accuracy)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(gm_maps_masked)\n",
    "type(age)\n",
    "type(GMtrain)\n",
    "type(GMtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gm_maps_masked.shape)\n",
    "print(GMtrain.shape)\n",
    "print(GMtest.shape)\n",
    "print(age.shape)\n",
    "print(age_train.shape)\n",
    "print(age_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random: Scree plot: a line plot of the eigenvalues of factors or principal components in an analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ANOVA + SVR\")\n",
    "# Define the prediction function to be used.\n",
    "# Here we use a Support Vector Classification, with a linear kernel\n",
    "from sklearn.svm import SVR\n",
    "svr = SVR(kernel='linear')\n",
    "\n",
    "# Dimension reduction\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, \\\n",
    "        f_regression\n",
    "\n",
    "# Remove features with too low between-subject variance\n",
    "variance_threshold = VarianceThreshold(threshold=.01)\n",
    "\n",
    "# Here we use a classical univariate feature selection based on F-test,\n",
    "# namely Anova.\n",
    "feature_selection = SelectKBest(f_regression, k=2000)\n",
    "\n",
    "# We have our predictor (SVR), our feature selection (SelectKBest), and now,\n",
    "# we can plug them together in a *pipeline* that performs the two operations\n",
    "# successively:\n",
    "from sklearn.pipeline import Pipeline\n",
    "anova_svr = Pipeline([\n",
    "            ('variance_threshold', variance_threshold),\n",
    "            ('anova', feature_selection),\n",
    "            ('svr', svr)])\n",
    "\n",
    "### Fit and predict\n",
    "anova_svr.fit(gm_maps_masked, age)\n",
    "age_pred = anova_svr.predict(gm_maps_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coef = svr.coef_\n",
    "# reverse feature selection\n",
    "coef = feature_selection.inverse_transform(coef)\n",
    "# reverse variance threshold\n",
    "coef = variance_threshold.inverse_transform(coef)\n",
    "# reverse masking\n",
    "weight_img = nifti_masker.inverse_transform(coef)\n",
    "\n",
    "# Create the figure\n",
    "from nilearn.plotting import plot_stat_map, show\n",
    "bg_filename = gray_matter_map_filenames[0]\n",
    "z_slice = 0\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(5.5, 7.5), facecolor='k')\n",
    "# Hard setting vmax to highlight weights more\n",
    "display = plot_stat_map(weight_img, bg_img=bg_filename,\n",
    "                        display_mode='z', cut_coords=[z_slice],\n",
    "                        figure=fig, vmax=1)\n",
    "display.title('SVM weights', y=1.2)\n",
    "\n",
    "# Measure accuracy with cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cv_scores = cross_val_score(anova_svr, gm_maps_masked, age)\n",
    "\n",
    "# Return the corresponding mean prediction accuracy\n",
    "prediction_accuracy = np.mean(cv_scores)\n",
    "print(\"=== ANOVA ===\")\n",
    "print(\"Prediction accuracy: %f\" % prediction_accuracy)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inference with massively univariate model #################################\n",
    "print(\"Massively univariate model\")# Statistical inference\n",
    "from nilearn.mass_univariate import permuted_ols\n",
    "data = variance_threshold.fit_transform(gm_maps_masked)\n",
    "neg_log_pvals, t_scores_original_data, _ = permuted_ols(\n",
    "    age, data,  # + intercept as a covariate by default\n",
    "    n_perm=2000,  # 1,000 in the interest of time; 10000 would be better\n",
    "    n_jobs=1)  # can be changed to use more CPUs\n",
    "signed_neg_log_pvals = neg_log_pvals * np.sign(t_scores_original_data)\n",
    "signed_neg_log_pvals_unmasked = nifti_masker.inverse_transform(\n",
    "    variance_threshold.inverse_transform(signed_neg_log_pvals))\n",
    "\n",
    "# Show results\n",
    "threshold = -np.log10(0.1)  # 10% corrected\n",
    "\n",
    "fig = plt.figure(figsize=(5.5, 7.5), facecolor='k')\n",
    "\n",
    "display = plot_stat_map(signed_neg_log_pvals_unmasked, bg_img=bg_filename,\n",
    "                        threshold=threshold, cmap=plt.cm.RdBu_r,\n",
    "                        display_mode='z', cut_coords=[z_slice],\n",
    "                        figure=fig)\n",
    "title = ('Negative $\\log_{10}$ p-values'\n",
    "         '\\n(Non-parametric + max-type correction)')\n",
    "display.title(title, y=1.2)\n",
    "\n",
    "n_detections = (signed_neg_log_pvals_unmasked.get_data() > threshold).sum()\n",
    "print('\\n%d detections' % n_detections)\n",
    "\n",
    "show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
